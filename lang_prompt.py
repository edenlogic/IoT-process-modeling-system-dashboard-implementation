"""
POSCO IoT AI ì„œë¹„ìŠ¤ - Gemini ë²„ì „ (ê°œì„ ë¨)
"""
import os
import json
import random
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging
from dotenv import load_dotenv

# Google Gemini imports
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("âš ï¸ Google Geminiê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install google-generativeai")

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê±° ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ===== ì„¤ì • ì„¹ì…˜ =====
class AIConfig:
    """AI ê´€ë ¨ ì„¤ì •"""
    ENABLED = os.getenv("ENABLE_AI_FEATURES", "false").lower() == "true"
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
    MODEL = os.getenv("GEMINI_MODEL", "gemini-pro")
    TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.3"))
    MAX_OUTPUT_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "500"))
    
    # ì•ˆì „ ì„¤ì •
    SAFETY_SETTINGS = [
        {
            "category": "HARM_CATEGORY_HARASSMENT",
            "threshold": "BLOCK_MEDIUM_AND_ABOVE"
        },
        {
            "category": "HARM_CATEGORY_HATE_SPEECH",
            "threshold": "BLOCK_MEDIUM_AND_ABOVE"
        },
        {
            "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            "threshold": "BLOCK_MEDIUM_AND_ABOVE"
        },
        {
            "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
            "threshold": "BLOCK_MEDIUM_AND_ABOVE"
        }
    ]
    
    # ìºì‹œ ì„¤ì •
    CACHE_ENABLED = os.getenv("AI_CACHE_ENABLED", "true").lower() == "true"
    CACHE_TTL = int(os.getenv("AI_CACHE_TTL", "300"))  # 5ë¶„

# ===== Mock ë°ì´í„° ì„¹ì…˜ =====
class MockData:
    """AI ì„œë¹„ìŠ¤ìš© ëª¨ì˜ ë°ì´í„°"""
    
    # ì„¤ë¹„ íŠ¹ì„±
    EQUIPMENT_CHARACTERISTICS = {
        "í”„ë ˆìŠ¤": {
            "critical_sensors": ["ì••ë ¥", "ì˜¨ë„"],
            "maintenance_cycle": "30ì¼",
            "risk_level": "ë†’ìŒ",
            "typical_issues": ["ì˜¤ì¼ ëˆ„ìœ ", "ì••ë ¥ ì €í•˜", "ê³¼ì—´"],
            "downtime_cost": "ì‹œê°„ë‹¹ 500ë§Œì›"
        },
        "ìš©ì ‘": {
            "critical_sensors": ["ì˜¨ë„", "ì „ë¥˜"],
            "maintenance_cycle": "45ì¼",
            "risk_level": "ì¤‘ê°„",
            "typical_issues": ["ì „ê·¹ ë§ˆëª¨", "ëƒ‰ê°ìˆ˜ ë¶€ì¡±", "ì „ì•• ë¶ˆì•ˆì •"],
            "downtime_cost": "ì‹œê°„ë‹¹ 300ë§Œì›"
        },
        "ì¡°ë¦½": {
            "critical_sensors": ["ì§„ë™", "ì†ë„"],
            "maintenance_cycle": "60ì¼",
            "risk_level": "ë‚®ìŒ",
            "typical_issues": ["ë²¨íŠ¸ ì¥ë ¥", "ëª¨í„° ê³¼ë¶€í•˜", "ì •ë ¬ ë¶ˆëŸ‰"],
            "downtime_cost": "ì‹œê°„ë‹¹ 200ë§Œì›"
        },
        "ê²€ì‚¬": {
            "critical_sensors": ["ì¹´ë©”ë¼", "ì„¼ì„œ"],
            "maintenance_cycle": "90ì¼",
            "risk_level": "ë‚®ìŒ",
            "typical_issues": ["ìº˜ë¦¬ë¸Œë ˆì´ì…˜", "ë Œì¦ˆ ì˜¤ì—¼", "ì¡°ëª… ë¶ˆëŸ‰"],
            "downtime_cost": "ì‹œê°„ë‹¹ 150ë§Œì›"
        },
        "í¬ì¥": {
            "critical_sensors": ["ì†ë„", "ìœ„ì¹˜"],
            "maintenance_cycle": "45ì¼",
            "risk_level": "ë‚®ìŒ",
            "typical_issues": ["ì»¨ë² ì´ì–´ ì¥ë ¥", "ì„¼ì„œ ì •ë ¬", "ëª¨í„° ê³¼ì—´"],
            "downtime_cost": "ì‹œê°„ë‹¹ 100ë§Œì›"
        }
    }
    
    # ì„¼ì„œ íŠ¸ë Œë“œ íŒ¨í„´
    SENSOR_TRENDS = {
        "ê¸‰ìƒìŠ¹": "ì§€ë‚œ 30ë¶„ê°„ {percent}% ìƒìŠ¹, ê°€ì†ë„ ì¦ê°€ ì¤‘",
        "ì ì§„ìƒìŠ¹": "ì§€ë‚œ 1ì‹œê°„ ë™ì•ˆ ì„œì„œíˆ {percent}% ìƒìŠ¹",
        "ë¶ˆì•ˆì •": "ìµœê·¼ 30ë¶„ê°„ Â±{percent}% ë²”ìœ„ì—ì„œ ë¶ˆê·œì¹™ ë³€ë™",
        "ê¸‰í•˜ë½": "ì§€ë‚œ 30ë¶„ê°„ {percent}% í•˜ë½, ê¸‰ê²©í•œ ê°ì†Œ",
        "ì •ìƒ": "ì§€ë‚œ 1ì‹œê°„ ë™ì•ˆ ì•ˆì •ì  ìœ ì§€ (Â±2% ì´ë‚´)"
    }
    
    # ê³¼ê±° ì‚¬ë¡€
    HISTORICAL_CASES = [
        {
            "situation": "í”„ë ˆìŠ¤ê¸° ì˜¨ë„ 85ë„ ì´ˆê³¼",
            "actions": [
                {"action": "interlock", "result": "ì„±ê³µ", "downtime": "2ì‹œê°„", "comment": "ëƒ‰ê° í›„ ì •ìƒí™”"},
                {"action": "bypass", "result": "ì‹¤íŒ¨", "downtime": "8ì‹œê°„", "comment": "ì¶”ê°€ ì†ìƒìœ¼ë¡œ ëŒ€í˜• ì •ë¹„"}
            ]
        },
        {
            "situation": "ìš©ì ‘ê¸° ì „ë¥˜ ë¶ˆì•ˆì •",
            "actions": [
                {"action": "bypass", "result": "ì„±ê³µ", "downtime": "0ì‹œê°„", "comment": "ìë™ ì•ˆì •í™”ë¨"},
                {"action": "interlock", "result": "ì„±ê³µ", "downtime": "1ì‹œê°„", "comment": "ì „ê·¹ êµì²´ í›„ í•´ê²°"}
            ]
        },
        {
            "situation": "ì¡°ë¦½ê¸° ì§„ë™ ì´ˆê³¼",
            "actions": [
                {"action": "interlock", "result": "ì„±ê³µ", "downtime": "30ë¶„", "comment": "ë²¨íŠ¸ ì¡°ì •ìœ¼ë¡œ í•´ê²°"},
                {"action": "bypass", "result": "ê²½ê³ ", "downtime": "0ì‹œê°„", "comment": "ëª¨ë‹ˆí„°ë§ ê°•í™” í•„ìš”"}
            ]
        },
        {
            "situation": "ê²€ì‚¬ê¸° ì„¼ì„œ ì´ìƒ",
            "actions": [
                {"action": "interlock", "result": "ì„±ê³µ", "downtime": "15ë¶„", "comment": "ìº˜ë¦¬ë¸Œë ˆì´ì…˜ìœ¼ë¡œ í•´ê²°"},
                {"action": "bypass", "result": "ì‹¤íŒ¨", "downtime": "4ì‹œê°„", "comment": "ë¶ˆëŸ‰í’ˆ ëŒ€ëŸ‰ ë°œìƒ"}
            ]
        }
    ]

# ===== í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¹ì…˜ =====
class Prompts:
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬"""
    
    ALERT_ANALYSIS = """ë‹¹ì‹ ì€ í¬ìŠ¤ì½”ëª¨ë¹Œë¦¬í‹° ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬ì˜ IoT ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í˜„ì¬ ì„¤ë¹„ì—ì„œ ì´ìƒ ì‹ í˜¸ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì·¨í•´ì•¼ í•  ì¡°ì¹˜ë¥¼ ì¶”ì²œí•˜ì„¸ìš”.

[í˜„ì¬ ìƒí™©]
ì„¤ë¹„: {equipment}
ì„¼ì„œ: {sensor_type} 
ì¸¡ì •ê°’: {value} (ì„ê³„ê°’: {threshold} ì´ˆê³¼)
ì„¼ì„œ íŠ¸ë Œë“œ: {trend}

[ì„¤ë¹„ ì •ë³´]
ì„¤ë¹„ íƒ€ì…: {equipment_type}
ì¤‘ìš” ì„¼ì„œ: {critical_sensors}
ì¼ë°˜ì  ë¬¸ì œ: {typical_issues}
ë‹¤ìš´íƒ€ì„ ë¹„ìš©: {downtime_cost}

[ìµœê·¼ ì´ë ¥]
- 24ì‹œê°„ ë‚´ ì•Œë¦¼: {alerts_24h}ê±´
- ìµœê·¼ ì£¼ìš” ë¬¸ì œ: {recent_issues}
- ë§ˆì§€ë§‰ ì •ë¹„: {last_maintenance}

[ìš”êµ¬ì‚¬í•­]
1. ì¸í„°ë½(ì¦‰ì‹œ ì •ì§€) ë˜ëŠ” ë°”ì´íŒ¨ìŠ¤(ì¼ì‹œ ë¬´ì‹œ) ì¤‘ í•˜ë‚˜ë¥¼ ëª…í™•íˆ ì¶”ì²œ
2. í•µì‹¬ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª… (1-2ë¬¸ì¥)
3. ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±:
   "ğŸš¨ ì¸í„°ë½ ê¶Œì¥ - [ì´ìœ ]" ë˜ëŠ” "â­ï¸ ë°”ì´íŒ¨ìŠ¤ ê°€ëŠ¥ - [ì´ìœ ]"

í˜„ì¥ ì‘ì—…ìê°€ ì¦‰ì‹œ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”."""

    ACTION_RECOMMENDATION = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ê³µì¥ ìš´ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í˜„ì¬ ì„¤ë¹„ ì´ìƒ ìƒí™©ì—ì„œ ìµœì ì˜ ì¡°ì¹˜ë¥¼ ì¶”ì²œí•´ì•¼ í•©ë‹ˆë‹¤.

[í˜„ì¬ ì•Œë¦¼ ìƒí™©]
ì„¤ë¹„: {equipment}
ë¬¸ì œ: {sensor_type} {value} (ì„ê³„ê°’ {threshold} ì´ˆê³¼)
ì‹¬ê°ë„: {severity}

[ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€]
{similar_cases}

[í˜„ì¬ ê³µì¥ ìƒí™©]
- ì „ì²´ ê°€ë™ë¥ : {factory_efficiency}
- ê°€ë™ ì¤‘ì¸ ì„¤ë¹„: {running_equipment}
- í˜„ì¬ ê·¼ë¬´ì¡°: {shift}
- ì„¤ë¹„ ìš°ì„ ìˆœìœ„: {priority}

[ì„ íƒ ê°€ëŠ¥í•œ ì¡°ì¹˜]
1. ì¸í„°ë½: ì„¤ë¹„ ì¦‰ì‹œ ì •ì§€
   - ì¥ì : ì•ˆì „ í™•ë³´, ì¶”ê°€ ì†ìƒ ë°©ì§€
   - ë‹¨ì : ìƒì‚° ì¤‘ë‹¨, ë‹¤ìš´íƒ€ì„ ë°œìƒ

2. ë°”ì´íŒ¨ìŠ¤: ì•Œë¦¼ ì¼ì‹œ ë¬´ì‹œí•˜ê³  ê³„ì† ê°€ë™
   - ì¥ì : ìƒì‚° ì§€ì†, ë‹¤ìš´íƒ€ì„ ì—†ìŒ
   - ë‹¨ì : ìœ„í—˜ ê°€ëŠ¥ì„±, ì†ìƒ í™•ëŒ€ ìœ„í—˜

ì •í™•íˆ ì•„ë˜ í˜•ì‹ì„ ë”°ë¼ ì‘ì„±í•˜ì„¸ìš”. ê° í•­ëª©ì€ ë°˜ë“œì‹œ ìƒˆ ì¤„ì—ì„œ ì‹œì‘í•˜ê³ , ì½œë¡ (:) ë’¤ì— ë‚´ìš©ì„ ì‘ì„±í•˜ì„¸ìš”:

ì¶”ì²œ_ì¡°ì¹˜: [ì¸í„°ë½ ë˜ëŠ” ë°”ì´íŒ¨ìŠ¤]
ì•ˆì „_ë¶„ì„: [í˜„ì¬ ìƒí™©ì˜ ìœ„í—˜ë„ì™€ ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”ì„±ì„ 1ë¬¸ì¥ìœ¼ë¡œ]
ì˜ˆìƒ_ê²°ê³¼: [ì¡°ì¹˜ ì‹œ ì˜ˆìƒë˜ëŠ” ê²°ê³¼ì™€ ì†Œìš” ì‹œê°„ì„ 1ë¬¸ì¥ìœ¼ë¡œ]
ê³¼ê±°_ë¹„êµ: [ìœ ì‚¬ ì‚¬ë¡€ì™€ ë¹„êµí•œ ë¶„ì„ì„ 1ë¬¸ì¥ìœ¼ë¡œ]

ì˜ˆì‹œ:
ì¶”ì²œ_ì¡°ì¹˜: ì¸í„°ë½
ì•ˆì „_ë¶„ì„: ì˜¨ë„ê°€ ì„ê³„ê°’ì„ 20% ì´ˆê³¼í•˜ì—¬ ì„¤ë¹„ ì†ìƒ ìœ„í—˜ì´ ë†’ì•„ ì¦‰ì‹œ ì •ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤
ì˜ˆìƒ_ê²°ê³¼: ì„¤ë¹„ ì •ì§€ í›„ 15-20ë¶„ ëƒ‰ê° ì‹œê°„ì„ ê±°ì³ ì •ìƒ ê°€ë™ ê°€ëŠ¥í•©ë‹ˆë‹¤
ê³¼ê±°_ë¹„êµ: ì§€ë‚œ 3ê°œì›”ê°„ ìœ ì‚¬ ìƒí™© 5ê±´ ì¤‘ 4ê±´ì´ ì¸í„°ë½ìœ¼ë¡œ 2ì‹œê°„ ë‚´ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤"""

# ===== ì‘ë‹µ íŒŒì„œ í´ë˜ìŠ¤ =====
class ResponseParser:
    """AI ì‘ë‹µì„ ì•ˆì •ì ìœ¼ë¡œ íŒŒì‹±í•˜ëŠ” í—¬í¼ í´ë˜ìŠ¤"""
    
    @staticmethod
    def parse_action_recommendation(content: str) -> dict:
        """ì¡°ì¹˜ ì¶”ì²œ ì‘ë‹µ íŒŒì‹±"""
        # ê¸°ë³¸ê°’
        result = {
            "action": "interlock",  # ì•ˆì „ì„ ìœ„í•œ ê¸°ë³¸ê°’
            "safety_analysis": "",
            "expected_result": "",
            "historical_comparison": ""
        }
        
        # ì •ê·œì‹ íŒ¨í„´ë“¤
        patterns = {
            "action": [
                r"ì¶”ì²œ_ì¡°ì¹˜\s*[:ï¼š]\s*(ì¸í„°ë½|ë°”ì´íŒ¨ìŠ¤|interlock|bypass)",
                r"ì¶”ì²œ\s*ì¡°ì¹˜\s*[:ï¼š]\s*(ì¸í„°ë½|ë°”ì´íŒ¨ìŠ¤|interlock|bypass)",
                r"ì¡°ì¹˜\s*[:ï¼š]\s*(ì¸í„°ë½|ë°”ì´íŒ¨ìŠ¤|interlock|bypass)",
            ],
            "safety": [
                r"ì•ˆì „_ë¶„ì„\s*[:ï¼š]\s*(.+?)(?=ì˜ˆìƒ_ê²°ê³¼|ê³¼ê±°_ë¹„êµ|$)",
                r"ì•ˆì „\s*ë¶„ì„\s*[:ï¼š]\s*(.+?)(?=ì˜ˆìƒ\s*ê²°ê³¼|ê³¼ê±°\s*ë¹„êµ|$)",
            ],
            "result": [
                r"ì˜ˆìƒ_ê²°ê³¼\s*[:ï¼š]\s*(.+?)(?=ê³¼ê±°_ë¹„êµ|$)",
                r"ì˜ˆìƒ\s*ê²°ê³¼\s*[:ï¼š]\s*(.+?)(?=ê³¼ê±°\s*ë¹„êµ|$)",
            ],
            "history": [
                r"ê³¼ê±°_ë¹„êµ\s*[:ï¼š]\s*(.+?)$",
                r"ê³¼ê±°\s*ë¹„êµ\s*[:ï¼š]\s*(.+?)$",
            ]
        }
        
        # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•œ ì¤„ë¡œ ë§Œë“¤ì–´ ì²˜ë¦¬
        content_oneline = content.replace('\n', ' ').strip()
        
        # ê° íŒ¨í„´ìœ¼ë¡œ ì‹œë„
        for pattern in patterns["action"]:
            match = re.search(pattern, content_oneline, re.IGNORECASE)
            if match:
                action_text = match.group(1).lower()
                if "ì¸í„°ë½" in action_text or "interlock" in action_text:
                    result["action"] = "interlock"
                elif "ë°”ì´íŒ¨ìŠ¤" in action_text or "bypass" in action_text:
                    result["action"] = "bypass"
                break
        
        # ì•ˆì „ ë¶„ì„ íŒŒì‹±
        for pattern in patterns["safety"]:
            match = re.search(pattern, content_oneline, re.IGNORECASE | re.DOTALL)
            if match:
                result["safety_analysis"] = match.group(1).strip()
                break
        
        # ì˜ˆìƒ ê²°ê³¼ íŒŒì‹±
        for pattern in patterns["result"]:
            match = re.search(pattern, content_oneline, re.IGNORECASE | re.DOTALL)
            if match:
                result["expected_result"] = match.group(1).strip()
                break
        
        # ê³¼ê±° ë¹„êµ íŒŒì‹±
        for pattern in patterns["history"]:
            match = re.search(pattern, content_oneline, re.IGNORECASE | re.DOTALL)
            if match:
                result["historical_comparison"] = match.group(1).strip()
                break
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë°±ì—… íŒŒì‹±
        if not result["action"]:
            if any(word in content.lower() for word in ["ì¸í„°ë½", "interlock", "ì •ì§€", "ì¤‘ë‹¨"]):
                result["action"] = "interlock"
            elif any(word in content.lower() for word in ["ë°”ì´íŒ¨ìŠ¤", "bypass", "ê³„ì†", "ë¬´ì‹œ"]):
                result["action"] = "bypass"
        
        logger.debug(f"íŒŒì‹± ê²°ê³¼: {result}")
        return result

# ===== ë©”ì¸ AI ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ =====
class AIService:
    """í†µí•© AI ì„œë¹„ìŠ¤ - Gemini ë²„ì „"""
    
    def __init__(self):
        self.enabled = AIConfig.ENABLED and GEMINI_AVAILABLE
        self.model = None
        self.cache = {}  # ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ìºì‹œ
        self.parser = ResponseParser()
        
        if self.enabled:
            self._initialize_gemini()
        else:
            logger.warning("AI ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def _initialize_gemini(self):
        """Gemini ì´ˆê¸°í™”"""
        try:
            genai.configure(api_key=AIConfig.GEMINI_API_KEY)
            
            # ëª¨ë¸ ì„¤ì •
            generation_config = {
                "temperature": AIConfig.TEMPERATURE,
                "top_p": 1,
                "top_k": 1,
                "max_output_tokens": AIConfig.MAX_OUTPUT_TOKENS,
            }
            
            self.model = genai.GenerativeModel(
                model_name=AIConfig.MODEL,
                generation_config=generation_config,
                safety_settings=AIConfig.SAFETY_SETTINGS
            )
            
            logger.info(f"Google Gemini ì´ˆê¸°í™” ì™„ë£Œ: {AIConfig.MODEL}")
            
        except Exception as e:
            logger.error(f"Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.enabled = False
    
    def _get_cache_key(self, prefix: str, data: dict) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_parts = [prefix]
        for k in sorted(['equipment', 'sensor_type', 'severity']):
            if k in data:
                key_parts.append(str(data[k]))
        return ":".join(key_parts)
    
    def _get_from_cache(self, key: str) -> Optional[str]:
        """ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°"""
        if not AIConfig.CACHE_ENABLED:
            return None
        
        cached = self.cache.get(key)
        if cached and datetime.now() < cached['expires']:
            logger.info(f"ìºì‹œ íˆíŠ¸: {key}")
            return cached['value']
        return None
    
    def _save_to_cache(self, key: str, value: str):
        """ìºì‹œì— ì €ì¥"""
        if AIConfig.CACHE_ENABLED:
            self.cache[key] = {
                'value': value,
                'expires': datetime.now() + timedelta(seconds=AIConfig.CACHE_TTL)
            }
            logger.info(f"ìºì‹œ ì €ì¥: {key}")
    
    def _clean_expired_cache(self):
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        now = datetime.now()
        expired_keys = [k for k, v in self.cache.items() if now > v['expires']]
        for key in expired_keys:
            del self.cache[key]
        if expired_keys:
            logger.info(f"ìºì‹œ ì •ë¦¬: {len(expired_keys)}ê°œ í•­ëª© ì‚­ì œ")
    
    def _generate_mock_context(self, alert_data: dict) -> dict:
        """Mock ë°ì´í„°ë¡œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        equipment_type = alert_data['equipment'].split('_')[0]
        equipment_type_kr = {
            "press": "í”„ë ˆìŠ¤", 
            "weld": "ìš©ì ‘", 
            "assemble": "ì¡°ë¦½",
            "inspect": "ê²€ì‚¬",
            "pack": "í¬ì¥"
        }.get(equipment_type, "ê¸°íƒ€")
        
        # ì„¤ë¹„ íŠ¹ì„±
        characteristics = MockData.EQUIPMENT_CHARACTERISTICS.get(
            equipment_type_kr, 
            MockData.EQUIPMENT_CHARACTERISTICS["í”„ë ˆìŠ¤"]
        )
        
        # ì„¼ì„œ íŠ¸ë Œë“œ (ì¸¡ì •ê°’/ì„ê³„ê°’ ë¹„ìœ¨ì— ë”°ë¼)
        ratio = alert_data['value'] / alert_data['threshold']
        if ratio > 1.2:
            trend = MockData.SENSOR_TRENDS["ê¸‰ìƒìŠ¹"].format(percent=random.randint(15, 30))
        elif ratio > 1.1:
            trend = MockData.SENSOR_TRENDS["ì ì§„ìƒìŠ¹"].format(percent=random.randint(5, 15))
        elif ratio < 0.8:
            trend = MockData.SENSOR_TRENDS["ê¸‰í•˜ë½"].format(percent=random.randint(10, 25))
        elif 0.9 < ratio < 1.1:
            trend = MockData.SENSOR_TRENDS["ì •ìƒ"]
        else:
            trend = MockData.SENSOR_TRENDS["ë¶ˆì•ˆì •"].format(percent=random.randint(5, 15))
        
        # ìµœê·¼ ì´ë ¥ (ëœë¤)
        recent_history = {
            "alerts_24h": random.randint(0, 5),
            "recent_issues": random.sample(
                ["ì˜¨ë„ ìƒìŠ¹", "ì••ë ¥ ì´ìƒ", "ì§„ë™ ì¦ê°€", "ì†ŒìŒ ë°œìƒ", "ì „ë ¥ ì†Œë¹„ ì¦ê°€"], 
                k=2
            ),
            "last_maintenance": (datetime.now() - timedelta(days=random.randint(5, 45))).strftime("%Y-%m-%d")
        }
        
        # ê³µì¥ í˜„í™© (ëœë¤)
        factory_status = {
            "efficiency": random.randint(85, 95),
            "running_equipment": f"{random.randint(12, 16)}/16",
            "shift": "ì£¼ê°„" if 6 <= datetime.now().hour < 18 else "ì•¼ê°„"
        }
        
        return {
            'equipment_type': equipment_type_kr,
            'characteristics': characteristics,
            'trend': trend,
            'recent_history': recent_history,
            'factory_status': factory_status
        }
    
    async def analyze_alert(self, alert_data: dict) -> str:
        """ì•Œë¦¼ ë¶„ì„ ë° ë©”ì‹œì§€ ìƒì„±"""
        if not self.enabled:
            return self._get_fallback_message(alert_data)
        
        # ìºì‹œ ì •ë¦¬
        self._clean_expired_cache()
        
        # ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key("alert", alert_data)
        cached = self._get_from_cache(cache_key)
        if cached:
            return cached
        
        try:
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = self._generate_mock_context(alert_data)
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = Prompts.ALERT_ANALYSIS.format(
                equipment=alert_data['equipment'],
                sensor_type=alert_data['sensor_type'],
                value=alert_data['value'],
                threshold=alert_data['threshold'],
                trend=context['trend'],
                equipment_type=context['equipment_type'],
                critical_sensors=", ".join(context['characteristics']['critical_sensors']),
                typical_issues=", ".join(context['characteristics']['typical_issues']),
                downtime_cost=context['characteristics']['downtime_cost'],
                alerts_24h=context['recent_history']['alerts_24h'],
                recent_issues=", ".join(context['recent_history']['recent_issues']),
                last_maintenance=context['recent_history']['last_maintenance']
            )
            
            # Gemini í˜¸ì¶œ
            response = self.model.generate_content(prompt)
            result = response.text.strip()
            
            # ì‘ë‹µ ê²€ì¦
            if len(result) < 10:
                raise ValueError("ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤")
            
            # ìºì‹œ ì €ì¥
            self._save_to_cache(cache_key, result)
            
            logger.info(f"AI ì•Œë¦¼ ë¶„ì„ ì™„ë£Œ: {alert_data['equipment']}")
            return result
            
        except Exception as e:
            logger.error(f"AI ì•Œë¦¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_fallback_message(alert_data)
    
    async def recommend_action(self, alert_data: dict) -> dict:
        """ì¡°ì¹˜ ì¶”ì²œ"""
        if not self.enabled:
            return self._get_fallback_recommendation()
        
        try:
            # Mock ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = self._generate_mock_context(alert_data)
            
            # ìœ ì‚¬ ì‚¬ë¡€ ì°¾ê¸°
            relevant_cases = []
            equipment_type = alert_data['equipment'].split('_')[0]
            
            for case in MockData.HISTORICAL_CASES:
                if equipment_type in case['situation'].lower() or \
                   alert_data['sensor_type'] in case['situation']:
                    relevant_cases.append(case)
            
            if not relevant_cases:
                relevant_cases = random.sample(MockData.HISTORICAL_CASES, k=2)
            
            # ìœ ì‚¬ ì‚¬ë¡€ í¬ë§·íŒ…
            cases_text = ""
            for i, case in enumerate(relevant_cases[:3], 1):
                cases_text += f"\nì‚¬ë¡€ {i}: {case['situation']}\n"
                for action in case['actions']:
                    cases_text += f"  - {action['action']}: {action['result']}, "
                    cases_text += f"ë‹¤ìš´íƒ€ì„ {action['downtime']}, {action['comment']}\n"
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = Prompts.ACTION_RECOMMENDATION.format(
                equipment=alert_data['equipment'],
                sensor_type=alert_data['sensor_type'],
                value=alert_data['value'],
                threshold=alert_data['threshold'],
                severity=alert_data['severity'],
                similar_cases=cases_text,
                factory_efficiency=f"{context['factory_status']['efficiency']}%",
                running_equipment=context['factory_status']['running_equipment'],
                shift=context['factory_status']['shift'],
                priority="ë†’ìŒ" if equipment_type in ["press", "weld"] else "ì¤‘ê°„"
            )
            
            # Gemini í˜¸ì¶œ
            response = self.model.generate_content(prompt)
            content = response.text.strip()
            
            # ì‘ë‹µ íŒŒì‹±
            parsed = self.parser.parse_action_recommendation(content)
            
            # íŒŒì‹± ê²°ê³¼ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ìƒì„±
            if not parsed["safety_analysis"]:
                ratio = alert_data['value'] / alert_data['threshold']
                if parsed["action"] == "interlock":
                    parsed["safety_analysis"] = f"{alert_data['sensor_type']} ì¸¡ì •ê°’ì´ ì„ê³„ê°’ì„ {(ratio-1)*100:.0f}% ì´ˆê³¼í•˜ì—¬ ì¦‰ì‹œ ì •ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤"
                else:
                    parsed["safety_analysis"] = f"{alert_data['sensor_type']} ìˆ˜ì¹˜ê°€ ê²½ë¯¸í•˜ê²Œ ìƒìŠ¹í–ˆìœ¼ë‚˜ ì¦‰ê°ì ì¸ ìœ„í—˜ì€ ì—†ìŠµë‹ˆë‹¤"
            
            if not parsed["expected_result"]:
                if parsed["action"] == "interlock":
                    parsed["expected_result"] = "ì„¤ë¹„ ì •ì§€ í›„ 10-30ë¶„ ë‚´ ì ê²€ ì™„ë£Œ ì˜ˆìƒ, ì•ˆì „ í™•ë³´ ê°€ëŠ¥"
                else:
                    parsed["expected_result"] = "ê³„ì† ëª¨ë‹ˆí„°ë§í•˜ë©° 30ë¶„ ë‚´ ìë™ ì•ˆì •í™” ì˜ˆìƒ"
            
            if not parsed["historical_comparison"]:
                if parsed["action"] == "interlock":
                    parsed["historical_comparison"] = "ê³¼ê±° ìœ ì‚¬ ìƒí™©ì—ì„œ ì¸í„°ë½ ì¡°ì¹˜ë¡œ í‰ê·  2ì‹œê°„ ë‚´ ì •ìƒí™”"
                else:
                    parsed["historical_comparison"] = "ì´ì „ 3ê±´ì˜ ìœ ì‚¬ ì‚¬ë¡€ì—ì„œ ë°”ì´íŒ¨ìŠ¤ í›„ ì •ìƒ ë³µê·€"
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = 0.85
            if "ì¦‰ì‹œ" in content or "ê¸´ê¸‰" in content or "ì‹¬ê°" in content:
                confidence = 0.95
            elif "ê²€í† " in content or "ëª¨ë‹ˆí„°ë§" in content:
                confidence = 0.75
            
            return {
                "action": parsed["action"],
                "explanation": content,
                "safety_analysis": parsed["safety_analysis"],
                "expected_result": parsed["expected_result"],
                "historical_comparison": parsed["historical_comparison"],
                "confidence": confidence,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"AI ì¡°ì¹˜ ì¶”ì²œ ì‹¤íŒ¨: {e}")
            return self._get_fallback_recommendation()
    
    def _get_fallback_message(self, alert_data: dict) -> str:
        """AI ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë©”ì‹œì§€"""
        sensor_kr = {
            "temperature": "ì˜¨ë„",
            "pressure": "ì••ë ¥",
            "vibration": "ì§„ë™",
            "power": "ì „ë ¥"
        }.get(alert_data['sensor_type'], alert_data['sensor_type'])
        
        # ê¸°ë³¸ ì¶”ì²œ ë¡œì§
        ratio = alert_data['value'] / alert_data['threshold']
        if ratio > 1.2:  # 20% ì´ìƒ ì´ˆê³¼
            return f"ğŸš¨ ì¸í„°ë½ ê¶Œì¥ - {sensor_kr} ì‹¬ê° ì´ˆê³¼, ì¦‰ì‹œ ì •ì§€ í•„ìš”"
        else:
            return f"â­ï¸ ë°”ì´íŒ¨ìŠ¤ ê°€ëŠ¥ - {sensor_kr} ê²½ë¯¸í•œ ì´ìƒ, ëª¨ë‹ˆí„°ë§ ê¶Œì¥"
    
    def _get_fallback_recommendation(self) -> dict:
        """AI ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì¶”ì²œ"""
        return {
            "action": "interlock",
            "explanation": "ì•ˆì „ì„ ìœ„í•´ ì„¤ë¹„ ì •ì§€ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤. AI ë¶„ì„ì´ ì¼ì‹œì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥í•˜ì—¬ ë³´ìˆ˜ì ì¸ ì ‘ê·¼ì„ ì¶”ì²œí•©ë‹ˆë‹¤.",
            "safety_analysis": "AI ë¶„ì„ ë¶ˆê°€ë¡œ ì•ˆì „ì„ ìš°ì„ ì‹œí•˜ì—¬ ì„¤ë¹„ ì •ì§€ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤",
            "expected_result": "ì„¤ë¹„ ì •ì§€ í›„ ìˆ˜ë™ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤",
            "historical_comparison": "ê³¼ê±° ë°ì´í„° ë¶„ì„ì´ ì¼ì‹œì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤",
            "confidence": 0.5,
            "timestamp": datetime.now().isoformat()
        }
    
    def get_status(self) -> dict:
        """AI ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        return {
            "enabled": self.enabled,
            "model": AIConfig.MODEL if self.enabled else None,
            "cache_size": len(self.cache),
            "api_key_set": bool(AIConfig.GEMINI_API_KEY),
            "timestamp": datetime.now().isoformat()
        }

# ===== ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ =====
ai_service = AIService()

# ===== í—¬í¼ í•¨ìˆ˜ë“¤ (FastAPIì—ì„œ ì§ì ‘ í˜¸ì¶œìš©) =====
async def generate_alert_message(alert_data: dict) -> str:
    """ì•Œë¦¼ ë©”ì‹œì§€ ìƒì„± (FastAPIìš©)"""
    return await ai_service.analyze_alert(alert_data)

async def get_action_recommendation(alert_data: dict) -> dict:
    """ì¡°ì¹˜ ì¶”ì²œ (FastAPIìš©)"""
    return await ai_service.recommend_action(alert_data)

def is_ai_enabled() -> bool:
    """AI ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€"""
    return ai_service.enabled

def get_ai_status() -> dict:
    """AI ì„œë¹„ìŠ¤ ìƒíƒœ"""
    return ai_service.get_status()

